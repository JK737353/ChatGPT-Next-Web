{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JK737353/ChatGPT-Next-Web/blob/main/gpt-sovitsV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "环境配置 environment"
      ],
      "metadata": {
        "id": "_o6a8GS2lWQM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9b7iFV3dm1f"
      },
      "source": [
        "!pip install -q condacolab\n",
        "# Setting up condacolab and installing packages\n",
        "import condacolab\n",
        "condacolab.install_from_url(\"https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh\")\n",
        "%cd -q /content\n",
        "!git clone https://github.com/RVC-Boss/GPT-SoVITS\n",
        "!conda install -y -q -c pytorch -c nvidia cudatoolkit\n",
        "%cd -q /content/GPT-SoVITS\n",
        "!conda install -y -q -c conda-forge gcc gxx ffmpeg cmake -c pytorch -c nvidia\n",
        "!/usr/local/bin/pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download pretrained models 下载预训练模型\n",
        "!mkdir -p /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/uvr5\n",
        "%cd /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!git clone https://huggingface.co/lj1995/GPT-SoVITS\n",
        "%cd /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\n",
        "# @title UVR5 pretrains 安装uvr5模型\n",
        "%cd /content/GPT-SoVITS/tools/uvr5\n",
        "%rm -r uvr5_weights\n",
        "!git clone https://huggingface.co/Delik/uvr5_weights\n",
        "!git config core.sparseCheckout true\n",
        "!mv /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/"
      ],
      "metadata": {
        "id": "0NgxXg5sjv7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title launch WebUI 启动WebUI\n",
        "!/usr/local/bin/pip install ipykernel\n",
        "!sed -i '10s/False/True/' /content/GPT-SoVITS/config.py\n",
        "%cd /content/GPT-SoVITS/\n",
        "!/usr/local/bin/python  webui.py"
      ],
      "metadata": {
        "id": "4oRGUzkrk8C7",
        "outputId": "0f28b6b1-0b38-4b70-8777-054abff76841",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipykernel\n",
            "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting comm>=0.1.1 (from ipykernel)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting debugpy>=1.6.5 (from ipykernel)\n",
            "  Downloading debugpy-1.8.13-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting ipython>=7.23.1 (from ipykernel)\n",
            "  Downloading ipython-8.18.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel)\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel)\n",
            "  Downloading jupyter_core-5.7.2-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting matplotlib-inline>=0.1 (from ipykernel)\n",
            "  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting nest-asyncio (from ipykernel)\n",
            "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from ipykernel) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/site-packages (from ipykernel) (7.0.0)\n",
            "Collecting pyzmq>=24 (from ipykernel)\n",
            "  Downloading pyzmq-26.3.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting tornado>=6.1 (from ipykernel)\n",
            "  Downloading tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting traitlets>=5.4.0 (from ipykernel)\n",
            "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting prompt-toolkit<3.1.0,>=3.0.41 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading prompt_toolkit-3.0.50-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (2.19.1)\n",
            "Collecting stack-data (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\n",
            "Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (8.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel) (3.21.0)\n",
            "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=7.23.1->ipykernel)\n",
            "  Downloading parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel)\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting wcwidth (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel)\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.17.0)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pure-eval (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading debugpy-1.8.13-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipython-8.18.1-py3-none-any.whl (808 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.2/808.2 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_core-5.7.2-py3-none-any.whl (28 kB)\n",
            "Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
            "Downloading pyzmq-26.3.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (867 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.3/867.3 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.2/437.2 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prompt_toolkit-3.0.50-py3-none-any.whl (387 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.8/387.8 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.7/103.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Installing collected packages: wcwidth, pure-eval, ptyprocess, traitlets, tornado, pyzmq, prompt-toolkit, pexpect, parso, nest-asyncio, executing, debugpy, asttokens, stack-data, matplotlib-inline, jupyter-core, jedi, comm, jupyter-client, ipython, ipykernel\n",
            "Successfully installed asttokens-3.0.0 comm-0.2.2 debugpy-1.8.13 executing-2.2.0 ipykernel-6.29.5 ipython-8.18.1 jedi-0.19.2 jupyter-client-8.6.3 jupyter-core-5.7.2 matplotlib-inline-0.1.7 nest-asyncio-1.6.0 parso-0.8.4 pexpect-4.9.0 prompt-toolkit-3.0.50 ptyprocess-0.7.0 pure-eval-0.2.3 pyzmq-26.3.0 stack-data-0.6.3 tornado-6.4.2 traitlets-5.14.3 wcwidth-0.2.13\n",
            "/content/GPT-SoVITS\n",
            "Downloading g2pw model...\n",
            "Extracting g2pw model...\n",
            "Running on local URL:  http://0.0.0.0:9874\n",
            "Running on public URL: https://10aaa7fe14bbc9d60a.gradio.live\n",
            "\"/usr/local/bin/python\" tools/slice_audio.py \"/content/GPT-SoVITS/impot/chattts-[seed_5790][speed_5][oral_9][laugh_0][break_4][2025-03-16_220506].wav\" \"output/slicer_opt\" -34 4000 300 10 500 0.9 0.25 0 1\n",
            "执行完毕，请检查输出文件\n",
            "\"/usr/local/bin/python\" tools/asr/funasr_asr.py -i \"output/slicer_opt\" -o \"output/asr_opt\" -s large -l zh -p float32\n",
            "2025-03-16 14:08:46,895 - modelscope - INFO - PyTorch version 2.6.0 Found.\n",
            "2025-03-16 14:08:46,896 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
            "2025-03-16 14:08:46,896 - modelscope - INFO - No valid ast index found from /root/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
            "2025-03-16 14:08:46,958 - modelscope - INFO - Loading done! Current index file version is 1.10.0, with md5 4e4b770f1627b90cd2dc8af9b574e473 and a total number of 946 components indexed\n",
            "2025-03-16 14:08:49,363 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 10.9k/10.9k [00:00<00:00, 1.04MB/s]\n",
            "Downloading: 100% 173k/173k [00:00<00:00, 574kB/s]\n",
            "Downloading: 100% 2.45k/2.45k [00:00<00:00, 947kB/s]\n",
            "Downloading: 100% 472/472 [00:00<00:00, 165kB/s]\n",
            "Downloading: 100% 840M/840M [00:14<00:00, 61.2MB/s]\n",
            "Downloading: 100% 19.1k/19.1k [00:00<00:00, 275kB/s]\n",
            "Downloading: 100% 7.90M/7.90M [00:00<00:00, 9.79MB/s]\n",
            "Downloading: 100% 48.7k/48.7k [00:00<00:00, 343kB/s]\n",
            "Downloading: 100% 91.5k/91.5k [00:00<00:00, 404kB/s]\n",
            "ckpt: /root/.cache/modelscope/hub/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/model.pt\n",
            "2025-03-16 14:09:25,130 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 7.85k/7.85k [00:00<00:00, 2.88MB/s]\n",
            "Downloading: 100% 1.19k/1.19k [00:00<00:00, 446kB/s]\n",
            "Downloading: 100% 365/365 [00:00<00:00, 135kB/s]\n",
            "Downloading: 100% 1.64M/1.64M [00:00<00:00, 2.98MB/s]\n",
            "Downloading: 100% 8.45k/8.45k [00:00<00:00, 3.11MB/s]\n",
            "Downloading: 100% 27.3k/27.3k [00:00<00:00, 385kB/s]\n",
            "Downloading: 100% 2.16M/2.16M [00:00<00:00, 3.93MB/s]\n",
            "ckpt: /root/.cache/modelscope/hub/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch/model.pt\n",
            "2025-03-16 14:09:37,605 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 6.00k/6.00k [00:00<00:00, 2.21MB/s]\n",
            "Downloading: 100% 810/810 [00:00<00:00, 302kB/s]\n",
            "Downloading: 100% 373/373 [00:00<00:00, 127kB/s]\n",
            "Downloading:  52% 144M/278M [00:06<00:05, 26.8MB/s]Speech Recognition Process Terminated\n",
            "\"/usr/local/bin/python\" tools/asr/funasr_asr.py -i \"/content/GPT-SoVITS/output\" -o \"/content/GPT-SoVITS/output/slicer_opt\" -s large -l zh -p float32\n",
            "2025-03-16 14:11:37,069 - modelscope - INFO - PyTorch version 2.6.0 Found.\n",
            "2025-03-16 14:11:37,070 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
            "2025-03-16 14:11:37,128 - modelscope - INFO - Loading done! Current index file version is 1.10.0, with md5 4e4b770f1627b90cd2dc8af9b574e473 and a total number of 946 components indexed\n",
            "2025-03-16 14:11:39,802 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "ckpt: /root/.cache/modelscope/hub/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/model.pt\n",
            "2025-03-16 14:11:45,513 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "ckpt: /root/.cache/modelscope/hub/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch/model.pt\n",
            "2025-03-16 14:11:47,048 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 278M/278M [00:08<00:00, 32.5MB/s]\n",
            "Downloading: 100% 863/863 [00:00<00:00, 327kB/s]\n",
            "Downloading: 100% 11.2k/11.2k [00:00<00:00, 3.76MB/s]\n",
            "Downloading: 100% 151k/151k [00:00<00:00, 498kB/s]\n",
            "Downloading: 100% 4.01M/4.01M [00:00<00:00, 6.13MB/s]\n",
            "ckpt: /root/.cache/modelscope/hub/iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/model.pt\n",
            "FunASR 模型加载完成: ZH\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "slicer_opt\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[ATraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/utils/load_utils.py\", line 93, in load_audio_text_image_video\n",
            "    data_or_path_or_list, audio_fs = torchaudio.load(data_or_path_or_list)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torchaudio/_backend/utils.py\", line 205, in load\n",
            "    return backend.load(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torchaudio/_backend/ffmpeg.py\", line 297, in load\n",
            "    return load_audio(uri, frame_offset, num_frames, normalize, channels_first, format)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torchaudio/_backend/ffmpeg.py\", line 88, in load_audio\n",
            "    s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torio/io/_streaming_media_decoder.py\", line 526, in __init__\n",
            "    self._be = ffmpeg_ext.StreamingMediaDecoder(os.path.normpath(src), format, option)\n",
            "RuntimeError: Failed to open the input \"/content/GPT-SoVITS/output/slicer_opt\" (Is a directory).\n",
            "Exception raised from get_input_format_context at /__w/audio/audio/pytorch/audio/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\n",
            "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7a54d456c1b6 in /usr/local/lib/python3.9/site-packages/torch/lib/libc10.so)\n",
            "frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7a54d4515a76 in /usr/local/lib/python3.9/site-packages/torch/lib/libc10.so)\n",
            "frame #2: <unknown function> + 0x42034 (0x7a53f08e9034 in /usr/local/lib/python3.9/site-packages/torio/lib/libtorio_ffmpeg4.so)\n",
            "frame #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x7a53f08eba34 in /usr/local/lib/python3.9/site-packages/torio/lib/libtorio_ffmpeg4.so)\n",
            "frame #4: <unknown function> + 0x3bb7e (0x7a53d973eb7e in /usr/local/lib/python3.9/site-packages/torio/lib/_torio_ffmpeg4.so)\n",
            "frame #5: <unknown function> + 0x32b5e (0x7a53d9735b5e in /usr/local/lib/python3.9/site-packages/torio/lib/_torio_ffmpeg4.so)\n",
            "frame #6: /usr/local/bin/python() [0x507387]\n",
            "frame #7: _PyObject_MakeTpCall + 0x2ec (0x4f073c in /usr/local/bin/python)\n",
            "frame #8: /usr/local/bin/python() [0x505313]\n",
            "frame #9: /usr/local/bin/python() [0x502a80]\n",
            "frame #10: /usr/local/bin/python() [0x4f0bea]\n",
            "frame #11: <unknown function> + 0xf7bb (0x7a53f096e7bb in /usr/local/lib/python3.9/site-packages/torchaudio/lib/_torchaudio.so)\n",
            "frame #12: _PyObject_MakeTpCall + 0x2ec (0x4f073c in /usr/local/bin/python)\n",
            "frame #13: _PyEval_EvalFrameDefault + 0x5263 (0x4ecc93 in /usr/local/bin/python)\n",
            "frame #14: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #15: _PyObject_FastCallDictTstate + 0x13e (0x4effae in /usr/local/bin/python)\n",
            "frame #16: /usr/local/bin/python() [0x5027bf]\n",
            "frame #17: _PyObject_MakeTpCall + 0x303 (0x4f0753 in /usr/local/bin/python)\n",
            "frame #18: _PyEval_EvalFrameDefault + 0x5263 (0x4ecc93 in /usr/local/bin/python)\n",
            "frame #19: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #20: _PyFunction_Vectorcall + 0xd4 (0x4f7e54 in /usr/local/bin/python)\n",
            "frame #21: _PyEval_EvalFrameDefault + 0x3c7 (0x4e7df7 in /usr/local/bin/python)\n",
            "frame #22: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #23: _PyFunction_Vectorcall + 0xd4 (0x4f7e54 in /usr/local/bin/python)\n",
            "frame #24: _PyEval_EvalFrameDefault + 0x4d34 (0x4ec764 in /usr/local/bin/python)\n",
            "frame #25: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #26: _PyFunction_Vectorcall + 0xd4 (0x4f7e54 in /usr/local/bin/python)\n",
            "frame #27: _PyEval_EvalFrameDefault + 0x4d34 (0x4ec764 in /usr/local/bin/python)\n",
            "frame #28: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #29: _PyFunction_Vectorcall + 0xd4 (0x4f7e54 in /usr/local/bin/python)\n",
            "frame #30: PyObject_Call + 0xb4 (0x5057d4 in /usr/local/bin/python)\n",
            "frame #31: _PyEval_EvalFrameDefault + 0x3e14 (0x4eb844 in /usr/local/bin/python)\n",
            "frame #32: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #33: _PyFunction_Vectorcall + 0xd4 (0x4f7e54 in /usr/local/bin/python)\n",
            "frame #34: _PyEval_EvalFrameDefault + 0x3c7 (0x4e7df7 in /usr/local/bin/python)\n",
            "frame #35: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #36: _PyFunction_Vectorcall + 0xd4 (0x4f7e54 in /usr/local/bin/python)\n",
            "frame #37: _PyEval_EvalFrameDefault + 0x1231 (0x4e8c61 in /usr/local/bin/python)\n",
            "frame #38: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #39: /usr/local/bin/python() [0x50508d]\n",
            "frame #40: PyObject_Call + 0xb4 (0x5057d4 in /usr/local/bin/python)\n",
            "frame #41: _PyEval_EvalFrameDefault + 0x3e14 (0x4eb844 in /usr/local/bin/python)\n",
            "frame #42: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #43: /usr/local/bin/python() [0x50508d]\n",
            "frame #44: PyObject_Call + 0xb4 (0x5057d4 in /usr/local/bin/python)\n",
            "frame #45: _PyEval_EvalFrameDefault + 0x3e14 (0x4eb844 in /usr/local/bin/python)\n",
            "frame #46: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #47: /usr/local/bin/python() [0x50508d]\n",
            "frame #48: PyObject_Call + 0xb4 (0x5057d4 in /usr/local/bin/python)\n",
            "frame #49: _PyEval_EvalFrameDefault + 0x3e14 (0x4eb844 in /usr/local/bin/python)\n",
            "frame #50: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #51: /usr/local/bin/python() [0x50508d]\n",
            "frame #52: _PyEval_EvalFrameDefault + 0x1231 (0x4e8c61 in /usr/local/bin/python)\n",
            "frame #53: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #54: _PyFunction_Vectorcall + 0xd4 (0x4f7e54 in /usr/local/bin/python)\n",
            "frame #55: _PyEval_EvalFrameDefault + 0x1231 (0x4e8c61 in /usr/local/bin/python)\n",
            "frame #56: /usr/local/bin/python() [0x4e6b2a]\n",
            "frame #57: _PyEval_EvalCodeWithName + 0x47 (0x4e67b7 in /usr/local/bin/python)\n",
            "frame #58: PyEval_EvalCodeEx + 0x39 (0x4e6769 in /usr/local/bin/python)\n",
            "frame #59: PyEval_EvalCode + 0x1b (0x59466b in /usr/local/bin/python)\n",
            "frame #60: /usr/local/bin/python() [0x5c1dc7]\n",
            "frame #61: /usr/local/bin/python() [0x5bddd0]\n",
            "frame #62: /usr/local/bin/python() [0x45674e]\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/utils/load_utils.py\", line 213, in _load_audio_ffmpeg\n",
            "    out = run(cmd, capture_output=True, check=True).stdout\n",
            "  File \"/usr/local/lib/python3.9/subprocess.py\", line 528, in run\n",
            "    raise CalledProcessError(retcode, process.args,\n",
            "subprocess.CalledProcessError: Command '['ffmpeg', '-nostdin', '-threads', '0', '-i', '/content/GPT-SoVITS/output/slicer_opt', '-f', 's16le', '-ac', '1', '-acodec', 'pcm_s16le', '-ar', '16000', '-']' returned non-zero exit status 235.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GPT-SoVITS/tools/asr/funasr_asr.py\", line 73, in execute_asr\n",
            "    text = model.generate(input=file_path)[0][\"text\"]\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/auto/auto_model.py\", line 248, in generate\n",
            "    return self.inference_with_vad(input, input_len=input_len, **cfg)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/auto/auto_model.py\", line 319, in inference_with_vad\n",
            "    res = self.inference(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/auto/auto_model.py\", line 285, in inference\n",
            "    res = model.inference(**batch, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/models/fsmn_vad_streaming/model.py\", line 676, in inference\n",
            "    audio_sample_list = load_audio_text_image_video(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/utils/load_utils.py\", line 72, in load_audio_text_image_video\n",
            "    return [\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/utils/load_utils.py\", line 73, in <listcomp>\n",
            "    load_audio_text_image_video(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/utils/load_utils.py\", line 97, in load_audio_text_image_video\n",
            "    data_or_path_or_list = _load_audio_ffmpeg(data_or_path_or_list, sr=fs)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/funasr/utils/load_utils.py\", line 215, in _load_audio_ffmpeg\n",
            "    raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
            "RuntimeError: Failed to load audio: ffmpeg version 7.0.1 Copyright (c) 2000-2024 the FFmpeg developers\n",
            "  built with gcc 12.3.0 (conda-forge gcc 12.3.0-7)\n",
            "  configuration: --prefix=/usr/local --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1716729489913/_build_env/bin/x86_64-conda-linux-gnu-cc --cxx=/home/conda/feedstock_root/build_artifacts/ffmpeg_1716729489913/_build_env/bin/x86_64-conda-linux-gnu-c++ --nm=/home/conda/feedstock_root/build_artifacts/ffmpeg_1716729489913/_build_env/bin/x86_64-conda-linux-gnu-nm --ar=/home/conda/feedstock_root/build_artifacts/ffmpeg_1716729489913/_build_env/bin/x86_64-conda-linux-gnu-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libharfbuzz --enable-libfontconfig --enable-libopenh264 --enable-libdav1d --enable-gnutls --enable-libmp3lame --enable-libvpx --enable-libass --enable-pthreads --enable-vaapi --enable-libopenvino --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1716729489913/_build_env/bin/pkg-config\n",
            "  libavutil      59.  8.100 / 59.  8.100\n",
            "  libavcodec     61.  3.100 / 61.  3.100\n",
            "  libavformat    61.  1.100 / 61.  1.100\n",
            "  libavdevice    61.  1.100 / 61.  1.100\n",
            "  libavfilter    10.  1.100 / 10.  1.100\n",
            "  libswscale      8.  1.100 /  8.  1.100\n",
            "  libswresample   5.  1.100 /  5.  1.100\n",
            "  libpostproc    58.  1.100 / 58.  1.100\n",
            "[in#0 @ 0x588bb213e2c0] Error opening input: Is a directory\n",
            "Error opening input file /content/GPT-SoVITS/output/slicer_opt.\n",
            "Error opening input files: Is a directory\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "100% 1/1 [00:00<00:00,  7.89it/s]\n",
            "ASR 任务完成->标注文件路径: /content/GPT-SoVITS/output/slicer_opt/output.list\n",
            "\n",
            "\"/usr/local/bin/python\" tools/subfix_webui.py --load_list \"/content/GPT-SoVITS/output/slicer_opt/output.list\" --webui_port 9871 --is_share True\n",
            "Running on local URL:  http://0.0.0.0:9871\n",
            "Running on public URL: https://6733de8b4c1ddf89f3.gradio.live\n",
            "\"/usr/local/bin/python\" tools/cmd-denoise.py -i \"/content/GPT-SoVITS/output\" -o \"/content/GPT-SoVITS/output/denoise_opt\" -p float16\n",
            "2025-03-16 14:14:57,070 - modelscope - INFO - PyTorch version 2.6.0 Found.\n",
            "2025-03-16 14:14:57,072 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
            "2025-03-16 14:14:57,109 - modelscope - INFO - Loading done! Current index file version is 1.10.0, with md5 4e4b770f1627b90cd2dc8af9b574e473 and a total number of 946 components indexed\n",
            "2025-03-16 14:15:03,934 - modelscope - WARNING - Model revision not specified, use revision: v1.0.2\n",
            "Downloading: 100% 1.45k/1.45k [00:00<00:00, 136kB/s]\n",
            "Downloading: 100% 903/903 [00:00<00:00, 71.6kB/s]\n",
            "Downloading: 100% 177k/177k [00:00<00:00, 584kB/s]\n",
            "Downloading: 100% 88.2k/88.2k [00:00<00:00, 392kB/s]\n",
            "Downloading: 100% 55.3M/55.3M [00:02<00:00, 24.0MB/s]\n",
            "Downloading: 100% 12.8k/12.8k [00:00<00:00, 1.39MB/s]\n",
            "Downloading: 100% 75.0k/75.0k [00:00<00:00, 330kB/s]\n",
            "Downloading: 100% 152k/152k [00:00<00:00, 540kB/s]\n",
            "2025-03-16 14:15:19,683 - modelscope - INFO - initiate model from /root/.cache/modelscope/hub/damo/speech_frcrn_ans_cirm_16k\n",
            "2025-03-16 14:15:19,683 - modelscope - INFO - initiate model from location /root/.cache/modelscope/hub/damo/speech_frcrn_ans_cirm_16k.\n",
            "2025-03-16 14:15:19,684 - modelscope - INFO - initialize model from /root/.cache/modelscope/hub/damo/speech_frcrn_ans_cirm_16k\n",
            "2025-03-16 14:15:20,981 - modelscope - WARNING - No preprocessor field found in cfg.\n",
            "2025-03-16 14:15:20,981 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
            "2025-03-16 14:15:20,981 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/root/.cache/modelscope/hub/damo/speech_frcrn_ans_cirm_16k'}. trying to build by task and model information.\n",
            "2025-03-16 14:15:20,981 - modelscope - WARNING - No preprocessor key ('speech_frcrn_ans_cirm_16k', 'acoustic-noise-suppression') found in PREPROCESSOR_MAP, skip building preprocessor.\n",
            "  0% 0/2 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/content/GPT-SoVITS/tools/cmd-denoise.py\", line 17, in execute_denoise\n",
            "    ans(\"%s/%s\"%(input_folder,name),output_path='%s/%s'%(output_folder,name))\n",
            "  File \"/usr/local/lib/python3.9/site-packages/modelscope/pipelines/base.py\", line 219, in __call__\n",
            "    output = self._process_single(input, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/modelscope/pipelines/base.py\", line 247, in _process_single\n",
            "    out = self.preprocess(input, **preprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/modelscope/pipelines/audio/ans_pipeline.py\", line 47, in preprocess\n",
            "    file_bytes = File.read(inputs)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/modelscope/fileio/file.py\", line 274, in read\n",
            "    return storage.read(uri)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/modelscope/fileio/file.py\", line 53, in read\n",
            "    with open(filepath, 'rb') as f:\n",
            "IsADirectoryError: [Errno 21] Is a directory: '/content/GPT-SoVITS/output/slicer_opt'\n",
            " 50% 1/2 [00:00<00:00,  3.33it/s]Traceback (most recent call last):\n",
            "  File \"/content/GPT-SoVITS/tools/cmd-denoise.py\", line 17, in execute_denoise\n",
            "    ans(\"%s/%s\"%(input_folder,name),output_path='%s/%s'%(output_folder,name))\n",
            "  File \"/usr/local/lib/python3.9/site-packages/modelscope/pipelines/base.py\", line 219, in __call__\n",
            "    output = self._process_single(input, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/modelscope/pipelines/base.py\", line 247, in _process_single\n",
            "    out = self.preprocess(input, **preprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/modelscope/pipelines/audio/ans_pipeline.py\", line 47, in preprocess\n",
            "    file_bytes = File.read(inputs)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/modelscope/fileio/file.py\", line 274, in read\n",
            "    return storage.read(uri)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/modelscope/fileio/file.py\", line 53, in read\n",
            "    with open(filepath, 'rb') as f:\n",
            "IsADirectoryError: [Errno 21] Is a directory: '/content/GPT-SoVITS/output/denoise_opt'\n",
            "100% 2/2 [00:00<00:00,  6.66it/s]\n",
            "\"/usr/local/bin/python\" tools/asr/funasr_asr.py -i \"/content/GPT-SoVITS/output/denoise_opt\" -o \"/content/GPT-SoVITS/output/slicer_opt\" -s large -l zh -p float32\n",
            "2025-03-16 14:18:29,450 - modelscope - INFO - PyTorch version 2.6.0 Found.\n",
            "2025-03-16 14:18:29,451 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
            "2025-03-16 14:18:29,487 - modelscope - INFO - Loading done! Current index file version is 1.10.0, with md5 4e4b770f1627b90cd2dc8af9b574e473 and a total number of 946 components indexed\n",
            "2025-03-16 14:18:31,641 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "ckpt: /root/.cache/modelscope/hub/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/model.pt\n",
            "2025-03-16 14:18:37,365 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "ckpt: /root/.cache/modelscope/hub/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch/model.pt\n",
            "2025-03-16 14:18:38,913 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "ckpt: /root/.cache/modelscope/hub/iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/model.pt\n",
            "FunASR 模型加载完成: ZH\n",
            "0it [00:00, ?it/s]\n",
            "ASR 任务完成->标注文件路径: /content/GPT-SoVITS/output/slicer_opt/denoise_opt.list\n",
            "\n",
            "\"/usr/local/bin/python\" tools/asr/funasr_asr.py -i \"/content/GPT-SoVITS/output/denoise_opt\" -o \"/content/GPT-SoVITS/output/slicer_opt\" -s large -l zh -p float32\n",
            "2025-03-16 14:19:42,319 - modelscope - INFO - PyTorch version 2.6.0 Found.\n",
            "2025-03-16 14:19:42,320 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
            "2025-03-16 14:19:42,356 - modelscope - INFO - Loading done! Current index file version is 1.10.0, with md5 4e4b770f1627b90cd2dc8af9b574e473 and a total number of 946 components indexed\n",
            "2025-03-16 14:19:44,525 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "ckpt: /root/.cache/modelscope/hub/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/model.pt\n",
            "2025-03-16 14:19:50,188 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "ckpt: /root/.cache/modelscope/hub/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch/model.pt\n",
            "2025-03-16 14:19:51,803 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "ckpt: /root/.cache/modelscope/hub/iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/model.pt\n",
            "FunASR 模型加载完成: ZH\n",
            "0it [00:00, ?it/s]\n",
            "ASR 任务完成->标注文件路径: /content/GPT-SoVITS/output/slicer_opt/denoise_opt.list\n",
            "\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "1.WAV_0000000000_0000292800.wav\n",
            "1.WAV_0000292800_0000382720.wav\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s2_train_v3_lora.py --config \"/content/GPT-SoVITS/TEMP/tmp_s2.json\"\n",
            "phoneme_data_len: 2\n",
            "wav_data_len: 100\n",
            "100% 100/100 [00:00<00:00, 80861.85it/s]\n",
            "skipped_phone:  0 , skipped_dur:  0\n",
            "total left:  100\n",
            "loaded pretrained GPT_SoVITS/pretrained_models/s2Gv3.pth <All keys matched successfully>\n",
            "start training from epoch 1\n",
            "100% 34/34 [01:14<00:00,  2.19s/it]\n",
            "100% 34/34 [00:27<00:00,  1.22it/s]\n",
            "training done\n",
            "[rank0]:[W316 14:46:40.806136567 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s1_train.py --config_file \"/content/GPT-SoVITS/TEMP/tmp_s1.yaml\" \n",
            "Seed set to 1234\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "<All keys matched successfully>\n",
            "ckpt_path: None\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "semantic_data_len: 2\n",
            "phoneme_data_len: 2\n",
            "                         item_name                                     semantic_audio\n",
            "0  1.WAV_0000000000_0000292800.wav  54 234 53 53 271 234 875 149 171 1003 950 100 ...\n",
            "1  1.WAV_0000292800_0000382720.wav  1012 967 693 872 448 555 261 10 803 302 740 76...\n",
            "dataset.__len__(): 100\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                 | Params | Mode \n",
            "-------------------------------------------------------\n",
            "0 | model | Text2SemanticDecoder | 77.6 M | train\n",
            "-------------------------------------------------------\n",
            "77.6 M    Trainable params\n",
            "0         Non-trainable params\n",
            "77.6 M    Total params\n",
            "310.426   Total estimated model params size (MB)\n",
            "257       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 14: 100% 7/7 [00:01<00:00,  3.62it/s, v_num=0, total_loss_step=3.23e+3, lr_step=0.002, top_3_acc_step=0.327, total_loss_epoch=8.31e+3, lr_epoch=0.002, top_3_acc_epoch=0.322]`Trainer.fit` stopped: `max_epochs=15` reached.\n",
            "Epoch 14: 100% 7/7 [00:24<00:00,  3.43s/it, v_num=0, total_loss_step=3.23e+3, lr_step=0.002, top_3_acc_step=0.327, total_loss_epoch=8.31e+3, lr_epoch=0.002, top_3_acc_epoch=0.322]\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/inference_webui.py \"Auto\"\n",
            "loading sovits_v3 <All keys matched successfully>\n",
            "Running on local URL:  http://0.0.0.0:9872\n",
            "Running on public URL: https://dc9eec9fe9954c64f1.gradio.live\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui.py\", line 533, in get_tts_wav\n",
            "    if (prompt_text[-1] not in splits): prompt_text += \"。\" if prompt_language != \"en\" else \".\"\n",
            "IndexError: string index out of range\n",
            "Actual Input Reference Text: 四川美食呢确实以辣闻名，但也有不辣的选择。比如什么甜水面啊、赖汤圆啊、蛋烘糕啊、叶儿粑等啊，这些小吃呢口味温和。\n",
            "Actual Input Target Text: 八角觉得自己赢麻了，可不是民进党在背后勾结赖哈模弄事情，蛙蛙开心死了\n",
            "Actual Input Target Text (after sentence segmentation): 八角觉得自己赢麻了，可不是民进党在背后勾结赖哈模弄事情，蛙蛙开心死了\n",
            "Actual Input Target Text (per sentence): 八角觉得自己赢麻了，可不是民进党在背后勾结赖哈模弄事情，蛙蛙开心死了。\n",
            "Processed text from the frontend (per sentence): 八角觉得自己赢麻了,可不是民进党在背后勾结赖哈模弄事情,蛙蛙开心死了.\n",
            "  8% 127/1500 [00:01<00:12, 106.35it/s]T2S Decoding EOS [243 -> 379]\n",
            "  9% 135/1500 [00:01<00:16, 81.22it/s] \n",
            "2.123\t8.422\t1.670\t4.541\n",
            "Actual Input Reference Text: 四川美食呢确实以辣闻名，但也有不辣的选择。比如什么甜水面啊、赖汤圆啊、蛋烘糕啊、叶儿粑等啊，这些小吃呢口味温和。\n",
            "Actual Input Target Text: 八角觉得自己赢麻了哈哈，可不是可不是呢民进党在背后勾结赖哈模弄事情，蛙蛙开心死了\n",
            "Actual Input Target Text (after sentence segmentation): 八角觉得自己赢麻了哈哈，可不是可不是呢民进党在背后勾结赖哈模弄事情，蛙蛙开心死了\n",
            "Actual Input Target Text (per sentence): 八角觉得自己赢麻了哈哈，可不是可不是呢民进党在背后勾结赖哈模弄事情，蛙蛙开心死了。\n",
            "Processed text from the frontend (per sentence): 八角觉得自己赢麻了哈哈,可不是可不是呢民进党在背后勾结赖哈模弄事情,蛙蛙开心死了.\n",
            " 13% 197/1500 [00:01<00:12, 101.04it/s]T2S Decoding EOS [243 -> 446]\n",
            " 13% 202/1500 [00:01<00:12, 103.76it/s]\n",
            "0.249\t1.533\t1.950\t4.191\n",
            "Actual Input Reference Text: 四川美食呢确实以辣闻名，但也有不辣的选择。比如什么甜水面啊、赖汤圆啊、蛋烘糕啊、叶儿粑等啊，这些小吃呢口味温和。\n",
            "Actual Input Target Text: 八角觉得自己赢麻了嘿嘿，可不是可不是呢民进党在背后勾结赖哈模弄事情，蛙蛙开心死了\n",
            "Actual Input Target Text (after sentence segmentation): 八角觉得自己赢麻了嘿嘿，可不是可不是呢民进党在背后勾结赖哈模弄事情，蛙蛙开心死了\n",
            "Actual Input Target Text (per sentence): 八角觉得自己赢麻了嘿嘿，可不是可不是呢民进党在背后勾结赖哈模弄事情，蛙蛙开心死了。\n",
            "Processed text from the frontend (per sentence): 八角觉得自己赢麻了嘿嘿,可不是可不是呢民进党在背后勾结赖哈模弄事情,蛙蛙开心死了.\n",
            " 12% 175/1500 [00:01<00:12, 105.67it/s]T2S Decoding EOS [243 -> 420]\n",
            " 12% 176/1500 [00:01<00:12, 102.07it/s]\n",
            "0.224\t1.448\t1.727\t3.570\n",
            "Actual Input Reference Text: 四川美食呢确实以辣闻名，但也有不辣的选择。比如什么甜水面啊、赖汤圆啊、蛋烘糕啊、叶儿粑等啊，这些小吃呢口味温和。\n",
            "Actual Input Target Text: 八角觉得自己赢麻了嘿嘿，可不是可不是呢民进党在背后勾结赖哈模弄事情，蛙蛙开心死了哈哈哈哈\n",
            "Actual Input Target Text (after sentence segmentation): 八角觉得自己赢麻了嘿嘿，可不是可不是呢民进党在背后勾结赖哈模弄事情，蛙蛙开心死了哈哈哈哈\n",
            "Actual Input Target Text (per sentence): 八角觉得自己赢麻了嘿嘿，可不是可不是呢民进党在背后勾结赖哈模弄事情，蛙蛙开心死了哈哈哈哈。\n",
            "Processed text from the frontend (per sentence): 八角觉得自己赢麻了嘿嘿,可不是可不是呢民进党在背后勾结赖哈模弄事情,蛙蛙开心死了哈哈哈哈.\n",
            " 14% 206/1500 [00:02<00:12, 103.42it/s]T2S Decoding EOS [243 -> 459]\n",
            " 14% 215/1500 [00:02<00:12, 100.74it/s]\n",
            "0.221\t1.638\t2.137\t3.978\n",
            "Actual Input Reference Text: 四川美食呢确实以辣闻名，但也有不辣的选择。比如什么甜水面啊、赖汤圆啊、蛋烘糕啊、叶儿粑等啊，这些小吃呢口味温和。\n",
            "Actual Input Target Text: 八角觉得自己赢麻了嘿嘿，可不是可不是呢民进党在背后勾结赖哈模弄事情，蛙蛙开心死了哈哈\n",
            "Actual Input Target Text (after sentence segmentation): 八角觉得自己赢麻了嘿嘿，可不是可不是呢民进党在背后勾结赖哈模弄事情，蛙蛙开心死了哈哈\n",
            "Actual Input Target Text (per sentence): 八角觉得自己赢麻了嘿嘿，可不是可不是呢民进党在背后勾结赖哈模弄事情，蛙蛙开心死了哈哈。\n",
            "Processed text from the frontend (per sentence): 八角觉得自己赢麻了嘿嘿,可不是可不是呢民进党在背后勾结赖哈模弄事情,蛙蛙开心死了哈哈.\n",
            " 13% 199/1500 [00:02<00:12, 102.49it/s]T2S Decoding EOS [243 -> 448]\n",
            " 14% 204/1500 [00:02<00:15, 82.48it/s] \n",
            "0.210\t2.488\t2.476\t3.840\n",
            "Actual Input Reference Text: 四川美食呢确实以辣闻名，但也有不辣的选择。比如什么甜水面啊、赖汤圆啊、蛋烘糕啊、叶儿粑等啊，这些小吃呢口味温和。\n",
            "Actual Input Target Text: 八角觉得自己赢麻了哈哈，可不是可不是呢民进党在背后勾结赖哈模弄事情，蛙蛙开心死了\n",
            "Actual Input Target Text (after sentence segmentation): 八角觉得自己赢麻了哈哈，可不是可不是呢民进党在背后勾结赖哈模弄事情，蛙蛙开心死了\n",
            "Actual Input Target Text (per sentence): 八角觉得自己赢麻了哈哈，可不是可不是呢民进党在背后勾结赖哈模弄事情，蛙蛙开心死了。\n",
            "Processed text from the frontend (per sentence): 八角觉得自己赢麻了哈哈,可不是可不是呢民进党在背后勾结赖哈模弄事情,蛙蛙开心死了.\n",
            " 12% 174/1500 [00:01<00:12, 105.94it/s]T2S Decoding EOS [243 -> 422]\n",
            " 12% 178/1500 [00:01<00:12, 103.60it/s]\n",
            "0.232\t1.445\t1.721\t3.653\n",
            "Actual Input Reference Text: 四川美食呢确实以辣闻名，但也有不辣的选择。比如什么甜水面啊、赖汤圆啊、蛋烘糕啊、叶儿粑等啊，这些小吃呢口味温和。\n",
            "Actual Input Target Text: 八角觉得自己赢麻了，那个可不是呢民进党什么的在背后勾结赖哈模弄事情，蛙蛙开心死了\n",
            "Actual Input Target Text (after sentence segmentation): 八角觉得自己赢麻了，那个可不是呢民进党什么的在背后勾结赖哈模弄事情，蛙蛙开心死了\n",
            "Actual Input Target Text (per sentence): 八角觉得自己赢麻了，那个可不是呢民进党什么的在背后勾结赖哈模弄事情，蛙蛙开心死了。\n",
            "Processed text from the frontend (per sentence): 八角觉得自己赢麻了,那个可不是呢民进党什么的在背后勾结赖哈模弄事情,蛙蛙开心死了.\n",
            " 11% 167/1500 [00:01<00:12, 108.03it/s]T2S Decoding EOS [243 -> 413]\n",
            " 11% 169/1500 [00:01<00:12, 105.61it/s]\n",
            "0.174\t1.704\t1.603\t3.513\n"
          ]
        }
      ]
    }
  ]
}